{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251ffb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sqlalchemy import create_engine, text\n",
    "import logging\n",
    "import time\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02706972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data (url , out):\n",
    "    try:        \n",
    "        #get the data from the url\n",
    "        url_data=requests.get(url)\n",
    "        #check if there was a response and write into a json file\n",
    "        if url_data.status_code== 200:\n",
    "            data=url_data.json()\n",
    "            with open (out , 'w') as f:\n",
    "                json.dump(data,f,indent=2)\n",
    "            #turn the json file into a dataframe\n",
    "            df = pd.read_json(out)\n",
    "            \n",
    "            logging.info(\"Data successfully fetched.\")\n",
    "            return df\n",
    "        else:\n",
    "            logging.error(f\"Failed to fetch data, code: {url_data.status_code}\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        logging.error(f\"Requests error: {str(req_err)}\")\n",
    "    \n",
    "    except json.JSONDecodeError as json_err:\n",
    "        logging.error(f\"JSON decoding error: {str(json_err)}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Error during fetching seen data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc221b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_unseen_data (database_name,table):\n",
    "    try:\n",
    "        # Create a SQLAlchemy engine\n",
    "        engine = create_engine(f\"mysql+mysqlconnector://root:password@localhost:3306/{database_name}\")\n",
    "\n",
    "        # Use pandas to read the table into a DataFrame\n",
    "        df = pd.read_sql_table(table, engine)\n",
    "\n",
    "        # Dispose of the engine\n",
    "        engine.dispose()\n",
    "        logging.info(\"Data successfully fetched.\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Error during fetching unseen data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e77a4e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df ,remove_outliers=True):\n",
    "    try:\n",
    "        df.columns = [str(col) for col in df.columns]\n",
    "        #sort the df attributes\n",
    "        df=df.sort_index(axis=1)\n",
    "        #drop duplicate rows\n",
    "        df = df.drop_duplicates()\n",
    "        #sort by timestamp\n",
    "        df = df.sort_values(by='Timestamp')\n",
    "        #do backward fill\n",
    "        df=df.bfill()\n",
    "        #drop not needed columns\n",
    "        columns_to_drop = ['source_ip', 'timestamp']\n",
    "        for column in columns_to_drop:\n",
    "            if column in df.columns:\n",
    "                df.drop(column, axis=1, inplace=True)\n",
    "        #get extra attributes from timestamp and drop it\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%d/%m/%Y %H:%M:%S')\n",
    "        df[\"hour\"] = df[\"Timestamp\"].dt.hour\n",
    "        df[\"minute\"] = df[\"Timestamp\"].dt.minute\n",
    "        df[\"day\"] = df[\"Timestamp\"].dt.day\n",
    "        df[\"month\"] = df[\"Timestamp\"].dt.month\n",
    "        df[\"year\"] = df[\"Timestamp\"].dt.year\n",
    "        df['seconds'] = df['Timestamp'].dt.second\n",
    "        df=df.drop(['Timestamp'], axis=1)\n",
    "        #remove outliers using iqr\n",
    "        if remove_outliers:\n",
    "            for columns in df.columns:\n",
    "                if columns not in ['Label','hour','minute','seconds','year','month','day']:\n",
    "                    try:\n",
    "                        Q1 = df[columns].quantile(0.25)\n",
    "                        Q3 = df[columns].quantile(0.75)\n",
    "                        IQR = Q3 - Q1\n",
    "                        outliers_index = ((df[columns] < (Q1 - 1.5 * IQR)) | (df[columns] > (Q3 + 1.5 * IQR)))\n",
    "                        df_iqr = df.loc[~outliers_index].reset_index(drop=True)\n",
    "                    except TypeError as e:\n",
    "                        print(f\"column: {columns} ,error :{e}\")\n",
    "            df = df_iqr\n",
    "            \n",
    "        #encode the label\n",
    "        label_encoder = LabelEncoder() \n",
    "        df['Label'] = label_encoder.fit_transform(df['Label'])\n",
    "        #scale the attributes \n",
    "        for columns in df.columns:\n",
    "            try:     \n",
    "                if columns not in ['Label', 'Timestamp','minute','hour','seconds','year','month','day','Dst Port']:\n",
    "                    scaler = StandardScaler()\n",
    "                    df[columns] = scaler.fit_transform(df[[columns]])\n",
    "            except TypeError as e:\n",
    "                print(f\"column: {columns} ,error :{e}\")\n",
    "        #create new pkt_ratio attribute from Tot Fwd Pkts and Tot Bwd Pkts\n",
    "        df['Pkt_Ratio'] = np.divide(df['Tot Fwd Pkts'], df['Tot Bwd Pkts'])\n",
    "        df['Pkt_Ratio'] = np.nan_to_num(df['Pkt_Ratio'], nan=0)\n",
    "        \n",
    "        logging.info(\"successfully transformed\\n\")\n",
    "        return df\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Error during data transformation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92184ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(df) :\n",
    "    try:\n",
    "        X = df.drop(['Label'], axis=1)\n",
    "        y = df['Label']\n",
    "        #train the model using holdout methos\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=123)\n",
    "        model.fit(X_train, y_train)\n",
    "        #predict label on the held out data\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        logging.info(\"Evaluating...\\n\")\n",
    "        #evaluating the predictions\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        classification_report_result = classification_report(y_test, y_pred)\n",
    "\n",
    "        logging.info(f'known data Accuracy: {accuracy:.5f}\\n')\n",
    "        logging.info('\\nClassification Report:\\n' + classification_report_result)\n",
    "        #using cross validation for training and evaluating\n",
    "        cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "        logging.info(\"Cross-Validation Scores for known data: \" + str(cv_scores))\n",
    "        logging.info(\"seen data Average Accuracy: \" + str(np.mean(cv_scores))+\"\\n\")\n",
    "\n",
    "        y_pred_cv = cross_val_predict(model, X, y, cv=5)\n",
    "        #print the confusion matrix\n",
    "        conf_matrix = confusion_matrix(y, y_pred_cv)\n",
    "        logging.info(\"Confusion Matrix for seen data:\\n\" + str(conf_matrix))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during data extraction: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e234d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(df_unseen_transformed , model):\n",
    "    try:\n",
    "        \n",
    "        #predict the unseen labels\n",
    "        model_prediction = model.predict(df_unseen_transformed.drop(columns=['Label']))\n",
    "        #check the accuracy of the model\n",
    "        accuracy_score_unseen = accuracy_score(df_unseen_transformed['Label'], model_prediction)\n",
    "        logging.info(f'The accuracy of the model: {accuracy_score_unseen:.5f}+\"\\n\"')\n",
    "        #create a confusion matrix\n",
    "        conf_matrix = confusion_matrix(df_unseen_transformed['Label'], model_prediction)\n",
    "        logging.info(\"Confusion Matrix for unseen data:\\n\" + str(conf_matrix))\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during prediction: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f9e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(df, database_name, table):\n",
    "    try:\n",
    "        #create the sql engine\n",
    "        engine = create_engine(\"mysql+mysqlconnector://root:password@localhost:3306\")\n",
    "        connection = engine.connect()\n",
    "        #create the database\n",
    "        create_database_query = text(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "        connection.execute(create_database_query)\n",
    "        #create the table from the df\n",
    "        engine = create_engine(f\"mysql+mysqlconnector://root:password@localhost:3306/{database_name}\")\n",
    "        df.to_sql(table, con=engine, if_exists='replace', index=False)\n",
    "        \n",
    "        logging.info(\"loaded succsessfully\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during data loading: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "556b90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "intrusion_url ='http://87.236.232.200:5000/data'\n",
    "output_file_name = \"intrusion_dataset.json\"\n",
    "prediction_data= pd.read_csv(\"intrusion_unseen.csv\")\n",
    "\n",
    "def monitor_pipeline():\n",
    "    while True:\n",
    "        try:\n",
    "            logging.info(\"Starting pipeline execution...\")\n",
    "            \n",
    "            logging.info(\"Extracting seen data...\")\n",
    "            df = fetch_data(intrusion_url , output_file_name)\n",
    "            \n",
    "            logging.info(\"loading unseen data...\")\n",
    "            load(prediction_data,'data','intrusion_unseen')\n",
    "            \n",
    "            logging.info(\"Extracting unseen data...\")\n",
    "            df_unseen=fetch_unseen_data('data','intrusion_unseen')\n",
    "            \n",
    "            logging.info(\"Transforming seen data...\")\n",
    "            df_transformed = transform(df)\n",
    "            \n",
    "            logging.info(\"Transforming unseen data...\")\n",
    "            df_unseen_transformed = transform(df_unseen,False)\n",
    "            \n",
    "            logging.info(\"classifying seen data...\")\n",
    "            model = classify(df_transformed)\n",
    "            \n",
    "            logging.info(\"predicting unseen data...\")\n",
    "            prediction(df_unseen_transformed , model)\n",
    "            \n",
    "            logging.info(\"loading data...\")\n",
    "            load(df_transformed,'data','engineer')\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Pipeline execution failed: {str(e)}\")\n",
    "            \n",
    "        logging.info(\"Waiting for the next iteration...\")\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4975729f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-10 13:25:15,765 - INFO - Starting pipeline execution...\n",
      "2024-01-10 13:25:15,766 - INFO - Extracting seen data...\n",
      "2024-01-10 13:25:17,306 - INFO - Data successfully fetched.\n",
      "2024-01-10 13:25:17,308 - INFO - loading unseen data...\n",
      "2024-01-10 13:25:17,821 - INFO - loaded succsessfully\n",
      "\n",
      "2024-01-10 13:25:17,822 - INFO - Extracting unseen data...\n",
      "2024-01-10 13:25:17,914 - INFO - Data successfully fetched.\n",
      "2024-01-10 13:25:17,915 - INFO - Transforming seen data...\n",
      "2024-01-10 13:25:18,509 - INFO - successfully transformed\n",
      "\n",
      "2024-01-10 13:25:18,510 - INFO - Transforming unseen data...\n",
      "2024-01-10 13:25:18,833 - INFO - successfully transformed\n",
      "\n",
      "2024-01-10 13:25:18,834 - INFO - classifying seen data...\n",
      "2024-01-10 13:25:19,117 - INFO - Evaluating...\n",
      "\n",
      "2024-01-10 13:25:19,130 - INFO - known data Accuracy: 1.00000\n",
      "\n",
      "2024-01-10 13:25:19,131 - INFO - \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        80\n",
      "           1       1.00      1.00      1.00        93\n",
      "           2       1.00      1.00      1.00        22\n",
      "\n",
      "    accuracy                           1.00       195\n",
      "   macro avg       1.00      1.00      1.00       195\n",
      "weighted avg       1.00      1.00      1.00       195\n",
      "\n",
      "2024-01-10 13:25:20,581 - INFO - Cross-Validation Scores for known data: [0.92307692 1.         1.         1.         0.98969072]\n",
      "2024-01-10 13:25:20,583 - INFO - seen data Average Accuracy: 0.9825535289452816\n",
      "\n",
      "2024-01-10 13:25:21,997 - INFO - Confusion Matrix for seen data:\n",
      "[[408   1   1]\n",
      " [ 15 424   0]\n",
      " [  0   0 124]]\n",
      "2024-01-10 13:25:21,998 - INFO - predicting unseen data...\n",
      "2024-01-10 13:25:22,018 - INFO - The accuracy of the model: 0.80000+\"\n",
      "\"\n",
      "2024-01-10 13:25:22,021 - INFO - Confusion Matrix for unseen data:\n",
      "[[6 0 0]\n",
      " [1 1 1]\n",
      " [0 0 1]]\n",
      "2024-01-10 13:25:22,021 - INFO - loading data...\n",
      "2024-01-10 13:25:23,147 - INFO - loaded succsessfully\n",
      "\n",
      "2024-01-10 13:25:23,148 - INFO - Waiting for the next iteration...\n"
     ]
    }
   ],
   "source": [
    "monitor_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d7b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
